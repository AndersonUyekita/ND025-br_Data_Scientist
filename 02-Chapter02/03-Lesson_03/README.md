# ND025 - Supervised Learning - Lesson 03

#### Tags
* Author : AH Uyekita
* Title  :  _Decision Trees_
* Date   : 18/03/2019
* Course : Data Scientist Nanodegree Program
    * COD    : ND025
    * **Instructor:** Luis Serrano

***

## Decision Trees

An example of this algorithm is the [Akinator Website][akinator_url], which works asking several questions and they could "predict" the person you are thinking.

[akinator_url]: https://en.akinator.com/game

Founded on this example above, let's try to create a recommender system. Table 1 shows the variables Gender and Occupation and the App downloaded by the user.

![Figure 1 - Recommending Apps.](01-img/nd025_c2_l03_01.png)

<center><em>Figure 1 - Recommending Apps.</em></center>

The two variables, gender and Occupation, should be used as inputs to "guess" the most likely App downloaded by these kind of person.

>Which variable should I use first?

or

>Between Gender and Occupation, which one seems more decisive of predicting what app will the users Download?

Let's have a look to the Gender.

![Figure 2 - Splitting by Gender.](01-img/nd025_c2_l03_02.png)

<center><em>Figure 2 - Splitting by Gender.</em></center>

Using `Gender` do not explain much about our aiming. Let's have a look splitting by Occupation.

![Figure 3 - Splitting by Occupation.](01-img/nd025_c2_l03_03.png)

<center><em>Figure 3 - Splitting by Occupation.</em></center>

Now, using the `Occupation` it is possible to visualize a clear pattern, when the user is a student, they usually download Pok√©mon Go. We *gain more information* using `Occupation`.

Continuing the split process, let's use the last variable.

![Figure 4 - Splitting by Occupation and later by Gender.](01-img/nd025_c2_l03_04.png)

<center><em>Figure 4 - Splitting by Occupation and later by Gender.</em></center>

You can divide the observation into 2 classes:

* female: prone to download WhatsApp, and;
* male: more likely to download Snapchat.

![Figure 5 - The Tree.](01-img/nd025_c2_l03_05.png)

<center><em>Figure 5 - The tree.</em></center>

The example above is about the classification process of using categorical variables/features. Now, let's try it using continuous features.

![Figure 6 - The Tree.](01-img/nd025_c2_l03_06.png)

<center><em>Figure 6 - Student Admissions.</em></center>

Where:

* `Blue`: Accepted, and;
* `Red`: Rejected.

Here, I have posed the same question.

> Between grades and test, which one determines students acceptance better?

Or rephrasing it.

> Between a horizontal and a vertical line, which one woould cut the data better?

In this time we need to determine the _threshold_, which provide the better split. Figure 7 shows the final Decision Tree.

![Figure 7 - The Tree.](01-img/nd025_c2_l03_07.png)

<center><em>Figure 7 - Student Admissions Decision Tree.</em></center>

The best feature to split in the first time is `test` and the optimized value is `5`. Later, there are two subsets, which I should perform the same procedure for each part. Thus, two new splits in `2` and `7` will be performed.

### Entropy

Have in mind, the entropy concept used in this subject is related to the [Information Theory][wiki_entropy_info], but has some similiraties with the Entropy of the Physics.

[wiki_entropy_info]: https://en.wikipedia.org/wiki/Entropy_(information_theory)

Here Entropy is a way to evaluate the split quality, so our focus here is to find an optimized way to divide the data into "two" groups. Each group could be evaluate by its "Knowledge", which is the "opposite" of entropy. Figure 8 shows an illustration of it.

![Figure 8 - Entropy.](01-img/nd025_c2_l03_08.png)

<center><em>Figure 8 - Entropy and Knowledge.</em></center><br>

From the left to right, the first bucket is homogeneous so the entropy is low because there is no messy in it. The second bucket is quite homogeneous also with only one ball blue, so the entropy is medium. Lastly, the third bucket has fifty-fity, which I could classify as high.



































.
